{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "5839c7f22d86b1dc00da8d7a68e0c4008c3f12c86e4bfc9632912e4139a59f9a"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def Jennrich(M1, M2, r: int):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    M1, M2: matrix of shape [d,d]\n",
    "    r: rank of M1 and M2\n",
    "    Outputs:\n",
    "    the eigenvectors of M1 * M2**(-1)\n",
    "    \"\"\"\n",
    "    U, _, __ = np.linalg.svd(M1)\n",
    "    W = U[:, 0 : r]\n",
    "    M1_whitened = W.T @ M1 @ W\n",
    "    M2_whitened = W.T @ M2 @ W\n",
    "    M = M1_whitened @ np.linalg.inv(M2_whitened)\n",
    "    e, P = np.linalg.eig(M)\n",
    "    return W @ P.real\n",
    "\n",
    "def matSys(A_r, Tx, x):\n",
    "    \"\"\"\n",
    "    Solve a matrix valued linear system A_r * Xi * diag(A_r.T * x) * A_r.T = Tx\n",
    "    Inputs: \n",
    "    A_r: [d,r]; \n",
    "    Tx: [d,d]; \n",
    "    x: [d,1]\n",
    "    Outputs:\n",
    "    Xi: [r,1]\n",
    "    \"\"\"\n",
    "    B = np.linalg.pinv(A_r).T\n",
    "    Xi = np.diagonal(B.T @ Tx @ B)\n",
    "    return Xi / np.tensordot(A_r, x, axes= [0,0])\n",
    "     # Original\n",
    "    # B = tf.transpose(tf.linalg.pinv(A_r))\n",
    "    # xi = tf.linalg.diag_part(tf.linalg.matmul(tf.linalg.matmul(tf.transpose(B), Tx), B))\n",
    "    # Xi = tf.divide(xi, tf.tensordot(tf.transpose(A_r), x, axes= [[1],[0]]))\n",
    "    # return Xi\n",
    "\n",
    "    # TODO: debug this method\n",
    "    # With Khatri-Rao and without pinv\n",
    "    # _, r = tf.shape(A_r)\n",
    "    # A = tf.reshape(tf.expand_dims(A_r,1) * tf.expand_dims(A_r,0), [-1, r])\n",
    "    # Xi = tf.linalg.lstsq(A, tf.reshape(Tx,[-1,1]))\n",
    "    # return Xi/tf.tensordot(A_r, x, axes = [0,0])\n",
    "\n",
    "    # # With Khatri-Rao\n",
    "    # _, r = tf.shape(A_r)\n",
    "    # A = tf.reshape(tf.expand_dims(A_r,1) * tf.expand_dims(A_r,0), [-1, r])\n",
    "    # Xi = tf.matmul(tf.linalg.pinv(A), tf.reshape(Tx,[-1,1]))\n",
    "    # return Xi/tf.tensordot(A_r, x, axes = [0,0])\n",
    "\n",
    "def decompose(T, x, y, r: int, timing = False):\n",
    "    \"\"\"\n",
    "    Given an input tensor, run Jennrich's algorithm and length recovery\n",
    "    Inputs:\n",
    "    T: tensor to be decomposed, of shape [d,d,d];\n",
    "    x,y: vectors to generate two inputs for Jennrich(), of shape [d,1]; \n",
    "    r: rank of T.\n",
    "    timing: a flag to control if to print computation time\n",
    "    Outputs:\n",
    "    A: a matrix with columns as directions of tensor components, of shape [d,r]\n",
    "    Xi: a vector containing the length of each conponent, of shape [r,1]\n",
    "    \"\"\"\n",
    "    if not timing:\n",
    "        T_x = np.tensordot(T, x, axes=[[0],[0]])\n",
    "        T_y = np.tensordot(T, y, axes=[[0],[0]])\n",
    "        A = Jennrich(T_x, T_y, r = r)\n",
    "        Xi = matSys(A, T_x, x)\n",
    "        return A, Xi\n",
    "    else:\n",
    "        T_x = np.tensordot(T, x, axes=[[0],[0]])\n",
    "        T_y = np.tensordot(T, y, axes=[[0],[0]])\n",
    "        \n",
    "        Jennrich_start_time = time.time()\n",
    "        A = Jennrich(T_x, T_y, r = r)\n",
    "        Jennrich_time = time.time() - Jennrich_start_time\n",
    "        print(\"Time to perform Jennrich's algorithm: {0:.5f}s\".format(Jennrich_time))\n",
    "\n",
    "        ls_start_time = time.time()\n",
    "        Xi = matSys(A, T_x, x)\n",
    "        ls_time = time.time() - ls_start_time\n",
    "        print(\"Time to solve the least squares problem: {0:.5f}s\".format(ls_time))\n",
    "        return A, Xi, Jennrich_time, ls_time\n",
    "    \n",
    "def constructSymTensor(components, weights = None):\n",
    "    \"\"\"\n",
    "    # Construct a 3rd order symmetric tensor with columns of components and muliplicative constants from weights\n",
    "    # Inputs:\n",
    "    # components: a matrix of shape [d,r]\n",
    "    # weights: a vector of shape [r, 1]\n",
    "    # Outputs:\n",
    "    # T = sum(weight[i] * component[i]@3) (where @ denotes the tensor product)\n",
    "    \"\"\"\n",
    "    _, rank = np.shape(components)\n",
    "    if weights is None:\n",
    "        weights = np.ones((1,rank))\n",
    "    return np.einsum('il,jl,kl->ijk', components * weights, components, components)\n",
    "    \n",
    "\n",
    "class OvercompleteTensorDecomposition():\n",
    "    \"\"\"\n",
    "    Overcomplete tensor decomposition\n",
    "    \"\"\"\n",
    "    def __init__(self, dimension, name = None):\n",
    "        self.x_magic, self.y_magic, self.x_2nd, self.y_2nd = np.random.multivariate_normal(np.zeros(dimension), np.identity(dimension), size = 4)\n",
    "    def __call__(self, T, tensor_rank, overcomplete_param, timing = False):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        T: a symmetric tensor of shape [d,d,d]\n",
    "        tensor_rank: rank of T\n",
    "        overcomplete_param: any (d - overcomplete_param) in T will be linearly independent. \n",
    "                            Set to tensor_rank - d for random tensors.\n",
    "        timing: a flag to control if to print computation time for each step\n",
    "        \"\"\"\n",
    "        # x = self.x_magic/tf.norm(self.x_magic)\n",
    "        # y = self.y_magic/tf.norm(self.y_magic)\n",
    "        if not timing:\n",
    "            # Decompose the first r compoents\n",
    "            A_r, Xi_r = decompose(T, self.x_magic, self.y_magic, r = tensor_rank - overcomplete_param)\n",
    "\n",
    "            # Deflation\n",
    "            T_first = constructSymTensor(components= A_r, weights= Xi_r)\n",
    "            R = T - T_first\n",
    "\n",
    "            # 2nd decomposition\n",
    "            A_k, Xi_k = decompose(R, self.x_2nd, self.y_2nd, r = overcomplete_param)\n",
    "            \n",
    "            # Reconstruction\n",
    "            T_second = constructSymTensor(components= A_k, weights= Xi_k)\n",
    "\n",
    "            return T_first + T_second, (A_r, Xi_r, A_k, Xi_k)\n",
    "        else:\n",
    "            # Decompose the first r compoents\n",
    "            print(\"First decomposition...\")\n",
    "            first_start_time = time.time()\n",
    "            A_r, Xi_r, Jennrich_time1, ls_time1 = decompose(T, self.x_magic, self.y_magic, r = tensor_rank - overcomplete_param, timing = True)\n",
    "\n",
    "            # Deflation\n",
    "            print(\"First reconstruction...\")\n",
    "            first_recon_start = time.time()\n",
    "            T_first = constructSymTensor(components= A_r, weights= Xi_r)\n",
    "            R = T - T_first\n",
    "            print(\"First reconstruction time: {:.5f}\".format(time.time() - first_recon_start))\n",
    "\n",
    "            # 2nd decomposition\n",
    "            print(\"Second decomposition...\")\n",
    "            A_k, Xi_k, Jennrich_time2, ls_time2 = decompose(R, self.x_2nd, self.y_2nd, r = overcomplete_param, timing = True)\n",
    "\n",
    "            # Reconstruction\n",
    "            print(\"Second reconstruction...\")\n",
    "            second_recon_start = time.time()\n",
    "            T_second = constructSymTensor(components= A_k, weights= Xi_k)\n",
    "            print(\"Second reconstruction time: {:.5f}\".format(time.time() - second_recon_start))\n",
    "\n",
    "            return T_first + T_second, (A_r, Xi_r, A_k, Xi_k), [Jennrich_time1, Jennrich_time2], [ls_time1, ls_time2]\n",
    "\n",
    "# TODO: debug, check broadcasting dimensions\n",
    "def tensorPowerIteration(T, k, max_steps = 20):\n",
    "    \"\"\"\n",
    "    Compute the components of T using tensor power iteration\n",
    "    Inputs:\n",
    "    T: a symmetric (orthogonal) tensor of shape [d,d,d]\n",
    "    k: rank of T\n",
    "    max_steps: maximal step in the iteration\n",
    "    Outputs:\n",
    "    A matrix of shape [d,k] containing the components of T in columns\n",
    "    \"\"\"\n",
    "    dim = T.shape[0]\n",
    "    res, weights = [], []\n",
    "    for i in range(k):\n",
    "        # intialization\n",
    "        x = np.expand_dims(np.random.multivariate_normal(np.zeros(dim), np.identity(dim)),1)\n",
    "        x /= np.linalg.norm(x)\n",
    "        # tensor power iterations\n",
    "        for step in range(max_steps):                                                   # compute T(I, x, x)/||T(I,x,x)||\n",
    "            x = np.expand_dims(np.tensordot(T, x @ x.T, axes = 2),1)\n",
    "            x /= np.linalg.norm(x)\n",
    "        # update & deflation\n",
    "        w = x.T @ np.tensordot(T, x @ x.T, axes = 2)                                    # get the factor\n",
    "        res.append(x[:,0])\n",
    "        weights.append(w)\n",
    "        T -= constructSymTensor(components = x, weights = w)                            # deflation\n",
    "    return np.array(res).T, np.array(weights).T                                         # return as a matrix\n",
    "\n",
    "def tensorTransform(T, A, B, C):\n",
    "    \"\"\"\n",
    "    Compute T[A,B,C]\n",
    "    \"\"\"\n",
    "    return np.einsum('mln,im,jl,kn->ijk', T, A, B, C)\n",
    "\n",
    "def tensorDecomposition(T, rank, timing = False):\n",
    "    \"\"\"\n",
    "    TODO: check why explosion happens\n",
    "    1. correctness of the transformation function       CHECKED\n",
    "    2. correctness of the orthogonalizer                                \n",
    "    3. correctness of the power iteration               CHECKED\n",
    "    4. correctness of the alternation\n",
    "    \"\"\"\n",
    "    dim, err, MAX_TRIAL = T.shape[0], 1, 100\n",
    "    # search for a good intialization\n",
    "    step = 0\n",
    "    while err > 1/2 and step < MAX_TRIAL:\n",
    "        roughDecomposer = OvercompleteTensorDecomposition(dim)\n",
    "        T_recovered, (A_r, Xi_r, A_k, Xi_k) = roughDecomposer(T, rank, rank-dim)\n",
    "        err = np.linalg.norm(T-T_recovered)/np.linalg.norm(T)\n",
    "        step += 1\n",
    "    # alternatively do tensor power iteration\n",
    "    step = 0\n",
    "    while err > 1e-1 and step < MAX_TRIAL:\n",
    "        # tensor power iteration on subproblem #1 (rank components)\n",
    "        if timing:\n",
    "            pwr1_start = time.time()\n",
    "        Q = np.linalg.pinv(A_r)                                                 # build an orthogonalizer for A_r\n",
    "        T2 = constructSymTensor(A_k, Xi_k)\n",
    "        T1 = tensorTransform(T - T2, Q, Q, Q)\n",
    "        A1, Xi_r = tensorPowerIteration(T1, dim)\n",
    "        A_r = normalize(A_r @ A1, axis = 0)\n",
    "        if timing:\n",
    "            pwr1_time = time.time() - pwr1_start \n",
    "        # tensor power iteration on subproblem #2 (rank-dim components)\n",
    "        if timing:\n",
    "            pwr2_start = time.time()\n",
    "        P = np.linalg.pinv(A_k)\n",
    "        T1 = constructSymTensor(A_r, Xi_r)\n",
    "        T2 = tensorTransform(T-T1, P, P, P)\n",
    "        A2, Xi_k = tensorPowerIteration(T2, rank - dim)\n",
    "        A_k = normalize(A_k @ A2, axis = 0)\n",
    "        if timing:\n",
    "            pwr2_time = time.time() - pwr2_start\n",
    "        # reconstruction\n",
    "        T_recovered = constructSymTensor(A_r, Xi_r) + constructSymTensor(A_k, Xi_k)\n",
    "        err = np.linalg.norm(T - T_recovered)\n",
    "        if step % 10 == 0:\n",
    "            print('Step {}: err = {:.5f}'.format(step, err))\n",
    "            if timing:\n",
    "                print('Time of: 1st subproblem {:.5f}, 2nd subproblem {:.5f}'.format(pwr1_time, pwr2_time))\n",
    "        step += 1\n",
    "    return err, (A_r, Xi_r, A_k, Xi_k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.array(range(1,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[[ 1  2  3]\n  [ 4  5  6]\n  [ 7  8  9]]\n\n [[10 11 12]\n  [13 14 15]\n  [16 17 18]]\n\n [[19 20 21]\n  [22 23 24]\n  [25 26 27]]]\n"
     ]
    }
   ],
   "source": [
    "T.shape = (3,3,3)\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = np.expand_dims(np.array([0,0,1]), axis=0)\n",
    "e2 = np.identity(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[25., 26., 27.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "tensorTransform(T,e1,e1,e2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[19, 20, 21],\n",
       "       [22, 23, 24],\n",
       "       [25, 26, 27]])"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "T[2,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = constructSymTensor(e2, weights= np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[[1., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 2., 0.],\n",
       "        [0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 3.]]])"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, W = tensorPowerIteration(T,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[2., 3., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}